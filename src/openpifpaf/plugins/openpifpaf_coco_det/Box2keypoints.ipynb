{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "import openpifpaf\n",
    "import numpy as np\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Json_Updating: \n",
    "\n",
    "    def __init__(self):\n",
    "        print('json file init')\n",
    "        self.json_file = {}\n",
    "\n",
    "\n",
    "    def initiate_json(self):\n",
    "        \"\"\"\n",
    "        Initiate json file: one for training phase and another one for validation.\n",
    "        \"\"\"\n",
    "        self.json_file[\"info\"] = dict(url=\"https://github.com/vita-epfl/openpifpaf\",\n",
    "                                    date_created=time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime()),\n",
    "                                    description=\"Conversion of ApolloCar3D dataset into MS-COCO format\")\n",
    "        \n",
    "        self.json_file[\"categories\"] = [] # Empty for initialization\n",
    "        self.json_file[\"images\"] = []  # Empty for initialization\n",
    "        self.json_file[\"annotations\"] = []  # Empty for initialization\n",
    "\n",
    "    def process_category(self, name, id, skeleton, supercategory, keypoints):\n",
    "        \"\"\"\n",
    "        Update image field in json file\n",
    "        \"\"\"\n",
    "        self.json_file[\"categories\"].append({\n",
    "            'name' : name,  # Category name\n",
    "            'id' : id,  # Id of category\n",
    "            'skeleton' : skeleton,  # Skeleton connections (check constants.py)\n",
    "            'supercategory' : supercategory,  # Same as category if no supercategory\n",
    "            'keypoints' : keypoints})\n",
    "\n",
    "    def process_image(self, images_anns):\n",
    "        \"\"\"\n",
    "        Update image field in json file\n",
    "        \"\"\"\n",
    "        # ------------------\n",
    "        # Add here your code\n",
    "        # -------------------\n",
    "        self.json_file[\"images\"].append(images_anns)\n",
    "        # self.json_file[\"images\"].append({\n",
    "        #     'coco_url': \"unknown\",\n",
    "        #     'file_name': '',  # Image name\n",
    "        #     'id': 0,  # Image id\n",
    "        #     'license': 1,  # License type\n",
    "        #     'date_captured': \"unknown\",  \n",
    "        #     'width': 0,  # Image width (pixels)\n",
    "        #     'height': 0})  # Image height (pixels)\n",
    "\n",
    "\n",
    "    def process_annotation(self, image_id, category_id, iscrowd, id, area, bbox, num_keypoints, keypoints, segmentation):\n",
    "        \"\"\"\n",
    "        Process and include in the json file a single annotation (instance) from a given image\n",
    "        \"\"\"\n",
    "        # ------------------\n",
    "        # Add here your code\n",
    "        # -------------------\n",
    "        self.json_file[\"annotations\"].append({\n",
    "            'image_id': image_id,  # Image id\n",
    "            'category_id': category_id,  # Id of the category (like car or person)\n",
    "            'iscrowd': iscrowd,  # 1 to mask crowd regions, 0 if the annotation is not a crowd annotation\n",
    "            'id': id,  # Id of the annotations\n",
    "            'area': area,  # Bounding box area of the annotation (width*height)\n",
    "            'bbox': bbox,  # Bounding box  coordinates (x0, y0, width, heigth), where x0, y0 are the left corner\n",
    "            'num_keypoints': num_keypoints,  # number of keypoints\n",
    "            'keypoints': keypoints,  # Flattened list of keypoints [x, y, visibility, x, y, visibility, .. ]\n",
    "            'segmentation': segmentation})  # To add a segmentation of the annotation, empty otherwise\n",
    "        \n",
    "        # self.json_file[\"annotations\"].append({\n",
    "        #     'image_id': 0,  # Image id\n",
    "        #     'category_id': 1,  # Id of the category (like car or person)\n",
    "        #     'iscrowd': 0,  # 1 to mask crowd regions, 0 if the annotation is not a crowd annotation\n",
    "        #     'id': 0,  # Id of the annotations\n",
    "        #     'area': 0,  # Bounding box area of the annotation (width*height)\n",
    "        #     'bbox': [],  # Bounding box  coordinates (x0, y0, width, heigth), where x0, y0 are the left corner\n",
    "        #     'num_keypoints': 0,  # number of keypoints\n",
    "        #     'keypoints': [],  # Flattened list of keypoints [x, y, visibility, x, y, visibility, .. ]\n",
    "        #     'segmentation': []})  # To add a segmentation of the annotation, empty otherwise\n",
    "\n",
    "\n",
    "    def save_json_files(self):\n",
    "\n",
    "        name = 'keypoints2box_test.json'\n",
    "        path_json = 'keypoints2box_test.json'\n",
    "        with open(path_json, 'w') as outfile:\n",
    "                json.dump(self.json_file, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Single class (person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "json file init\n"
     ]
    }
   ],
   "source": [
    "ann_path = 'data-mscoco/small_dataset/train/person_keypoints_small_train2017_multi_cat.json'\n",
    "\n",
    "coco = COCO(ann_path)\n",
    "json_file = Json_Updating()\n",
    "\n",
    "category_ids = coco.getCatIds()\n",
    "\n",
    "# Attention il va manquer l'initialisation de 'categories\"\n",
    "#json_file.initiate_json(name = 'person', id = category_ids[0], skeleton = [[1, 2], [1, 3], [1, 4], [1, 5], [2, 3], [2, 4], [3, 5], [5, 4]], supercategory = 'person', keypoints = ['box_center','box_left_up_corner','box_right_up_corner','box_left_down_corner','box_right_down_corner'])\n",
    "\n",
    "ids = coco.getImgIds(catIds=category_ids)\n",
    "#print(ids)\n",
    "\n",
    "for image_id in ids:\n",
    "\n",
    "    # Get ids from annotation and categories \n",
    "    ann_ids = coco.getAnnIds(imgIds=image_id, catIds=category_ids)\n",
    "    #cat_ids = coco.getCatIds()\n",
    "\n",
    "    ########### Images extraction ###########\n",
    "\n",
    "    # Extract the data of annotation, images and categories\n",
    "    images = coco.loadImgs(image_id)\n",
    "    \n",
    "    # Write the images anns in the json\n",
    "    json_file.process_image(images[0])\n",
    "    \n",
    "\n",
    "    ########### annotation extraction ###########\n",
    "\n",
    "    # load anns from the json file\n",
    "    anns = coco.loadAnns(ann_ids) # load segemntation data\n",
    "    #anns = [ann for ann in anns if not ann.get('iscrowd')]\n",
    "    \n",
    "    # extract kps from the anns\n",
    "    #kp_anns = [ann for ann in anns if 'keypoints' in ann and any(v > 0.0 for v in ann['keypoints'][2::3])] # Check si keypoint présents et > 0\n",
    "    #p_anns = [ann.get('keypoints') for ann in kp_anns] #Load the kp\n",
    "\n",
    "    # extract boxes from the anns\n",
    "    box_anns = [ann for ann in anns if 'bbox' in ann and any(v > 0.0 for v in ann['bbox'])] # Check si il y a au moins 1 bbox > 0\n",
    "    box_anns = [ann.get('bbox') for ann in box_anns]\n",
    "\n",
    "    # Create new boxes\n",
    "    if len(box_anns) > 0:\n",
    "        new_box_anns = []\n",
    "        box = box_anns[0]\n",
    "        b0, b1, w, h = box[0], box[1], box[2], box[3]\n",
    "        box_vals = [round((b0 + w)/2.0, 2), round((b1+h)/2.0, 2), 2., round(b0,2), round(b1,2), 2., round(b0+w,2), round(b1,2), 2., round(b0,2), round(b1+h,2), 2., round(b0+w,2), round(b1+h,2), 2.]\n",
    "        for i in range(len(box_vals)): new_box_anns.append(box_vals[i])\n",
    "    else : \n",
    "        new_box_anns = [0 for i in range(15)] # Put new boxes to 0 if there is no bounding boxes\n",
    "        box_anns = [[0,0,0,0]]\n",
    "\n",
    "    # extract other non changing annotations to copy inside the json file from the anns\n",
    "    segm_anns = [ann.get('segmentation') for ann in anns]\n",
    "    num_keypoints_anns = len(new_box_anns)/3\n",
    "    area_anns = [ann.get('area') for ann in anns]\n",
    "    iscrowd_anns = [ann.get('iscrowd') for ann in anns]\n",
    "    id_anns = [ann.get('id') for ann in anns]\n",
    "    category_id_anns = [ann.get('category_id') for ann in anns]\n",
    "\n",
    "\n",
    "    json_file.process_annotation(image_id, category_id_anns[0], iscrowd_anns[0], id_anns[0], area_anns[0], box_anns[0], num_keypoints_anns, new_box_anns, segm_anns[0])\n",
    "\n",
    "\n",
    "json_file.save_json_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi class code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "json file init\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n",
      "cat :  0\n",
      "0\n",
      "cat :  0\n",
      "1\n",
      "cat :  0\n",
      "2\n",
      "cat :  0\n",
      "3\n",
      "cat :  0\n",
      "4\n",
      "cat :  0\n",
      "5\n",
      "cat :  0\n",
      "6\n",
      "cat :  0\n",
      "7\n",
      "cat :  0\n",
      "8\n",
      "cat :  0\n",
      "9\n",
      "cat :  0\n",
      "10\n",
      "cat :  0\n",
      "11\n",
      "cat :  0\n",
      "12\n",
      "cat :  0\n",
      "13\n",
      "cat :  0\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "ann_path = 'data-mscoco/small_dataset/train/person_keypoints_small_train2017_multi_cat.json'\n",
    "\n",
    "coco = COCO(ann_path)\n",
    "json_file = Json_Updating()\n",
    "\n",
    "category_ids = coco.getCatIds()\n",
    "#print(len(category_ids))\n",
    "\n",
    "json_file.initiate_json()\n",
    "\n",
    "\n",
    "cat_ids = -1\n",
    "for cat in category_ids:\n",
    "    cat_ids = cat_ids + 1\n",
    "    categories = images = coco.loadCats(cat)\n",
    "\n",
    "    name = [cats.get('name') for cats in categories]\n",
    "    id = [cats.get('id') for cats in categories]\n",
    "    supercategory = [cats.get('supercategory') for cats in categories]\n",
    "\n",
    "    keypoints = [f'{name[0]}_box_center',f'{name[0]}_box_left_up_corner',f'{name[0]}_box_right_up_corner',f'{name[0]}_box_left_down_corner',f'{name[0]}_box_right_down_corner']\n",
    "    cat_mult = 5*(cat_ids)\n",
    "    skeleton = [[1 + cat_mult, 2 + cat_mult], [1 + cat_mult, 3 + cat_mult], [1 + cat_mult, 4 + cat_mult], [1 + cat_mult, 5 + cat_mult], [2 + cat_mult, 3 + cat_mult], [2 + cat_mult, 4 + cat_mult], [3 + cat_mult, 5 + cat_mult], [5 + cat_mult, 4 + cat_mult]]\n",
    "\n",
    "    json_file.process_category(name[0], id[0], skeleton, supercategory[0], keypoints)\n",
    "    \n",
    "    ids = coco.getImgIds(catIds=cat)\n",
    "    #print(ids)\n",
    "\n",
    "    box_80 = [0 for i in range(len(category_ids)*15)]\n",
    "\n",
    "    for image_id in ids:\n",
    "        # Get ids from annotation and categories \n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id, catIds=cat)\n",
    "        #cat_ids = coco.getCatIds()\n",
    "\n",
    "        ########### Images extraction ###########\n",
    "\n",
    "        # Extract the data of annotation, images and categories\n",
    "        images = coco.loadImgs(image_id)\n",
    "        \n",
    "        # Write the images anns in the json\n",
    "        json_file.process_image(images[0])\n",
    "        \n",
    "\n",
    "        ########### annotation extraction ###########\n",
    "\n",
    "        # load anns from the json file\n",
    "        anns = coco.loadAnns(ann_ids) # load segemntation data\n",
    "        #anns = [ann for ann in anns if not ann.get('iscrowd')]\n",
    "        \n",
    "        # extract kps from the anns\n",
    "        #kp_anns = [ann for ann in anns if 'keypoints' in ann and any(v > 0.0 for v in ann['keypoints'][2::3])] # Check si keypoint présents et > 0\n",
    "        #p_anns = [ann.get('keypoints') for ann in kp_anns] #Load the kp\n",
    "\n",
    "        # extract boxes from the anns\n",
    "        box_anns = [ann for ann in anns if 'bbox' in ann and any(v > 0.0 for v in ann['bbox'])] # Check si il y a au moins 1 bbox > 0\n",
    "        box_anns = [ann.get('bbox') for ann in box_anns]\n",
    "\n",
    "        # Create new boxes\n",
    "        if len(box_anns) > 0:\n",
    "            new_box_anns = []\n",
    "            box = box_anns[0]\n",
    "            b0, b1, w, h = box[0], box[1], box[2], box[3]\n",
    "            box_vals = [round((b0 + w)/2.0, 2), round((b1+h)/2.0, 2), 2., round(b0,2), round(b1,2), 2., round(b0+w,2), round(b1,2), 2., round(b0,2), round(b1+h,2), 2., round(b0+w,2), round(b1+h,2), 2.]\n",
    "            for i in range(len(box_vals)): new_box_anns.append(box_vals[i])\n",
    "        else : \n",
    "            new_box_anns = [0 for i in range(15)] # Put new boxes to 0 if there is no bounding boxes\n",
    "            box_anns = [[0,0,0,0]]\n",
    "\n",
    "        # put box in the form of 80 categories boxes\n",
    "        for i in range(15): \n",
    "            box_80[i + (cat_ids)*15] = new_box_anns[i]\n",
    "\n",
    "        # extract other non changing annotations to copy inside the json file from the anns\n",
    "        segm_anns = [ann.get('segmentation') for ann in anns]\n",
    "        num_keypoints_anns = len(new_box_anns)/3\n",
    "        area_anns = [ann.get('area') for ann in anns]\n",
    "        iscrowd_anns = [ann.get('iscrowd') for ann in anns]\n",
    "        id_anns = [ann.get('id') for ann in anns]\n",
    "        category_id_anns = [ann.get('category_id') for ann in anns]\n",
    "\n",
    "\n",
    "        json_file.process_annotation(image_id, category_id_anns[0], iscrowd_anns[0], id_anns[0], area_anns[0], box_anns[0], num_keypoints_anns, box_80, segm_anns[0])\n",
    "\n",
    "\n",
    "json_file.save_json_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
